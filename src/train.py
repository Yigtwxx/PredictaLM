# python src/train.py
# Sadece bu komutla Ã§alÄ±ÅŸtÄ±r: python src/train.py

import os
import csv
import math
import time
import torch
from torch.utils.data import DataLoader, random_split
from torch.optim import AdamW

from tokenizer import WordTokenizer
from dataset import LMTextDataset, collate_batch
from model import MiniGPT, MiniGPTConfig
from pynvml import *

nvmlInit()
handle = nvmlDeviceGetHandleByIndex(0)  # 0 = ilk GPU


def build_mixed_corpus(wiki_path, chat_path, out_path,
                       wiki_repeat=3, chat_repeat=1,
                       wiki_limit_lines=None):
    """
    wiki + konuÅŸma verisini tek bir dosyada birleÅŸtirir.
    - Wiki satÄ±rlarÄ±nÄ± wiki_repeat kez (default: 3)
    - Chat satÄ±rlarÄ±nÄ± chat_repeat kez (default: 1)
    tekrarlar.
    """
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    wiki_exists = os.path.exists(wiki_path)
    chat_exists = os.path.exists(chat_path)

    if not wiki_exists and not chat_exists:
        raise FileNotFoundError(
            f"Neither wiki file nor chat file exists.\n"
            f"wiki_path={wiki_path}\nchat_path={chat_path}"
        )

    wiki_lines_used = 0
    chat_lines_used = 0

    with open(out_path, "w", encoding="utf-8") as fout:
        # --- WIKI KISMI ---
        if wiki_exists:
            print(f"ðŸ“š Using wiki data from: {wiki_path}")
            with open(wiki_path, "r", encoding="utf-8") as f:
                for i, line in enumerate(f):
                    if wiki_limit_lines is not None and i >= wiki_limit_lines:
                        break
                    line = line.rstrip("\n")
                    if not line.strip():
                        continue
                    for _ in range(wiki_repeat):
                        fout.write(line + "\n")
                    wiki_lines_used += 1
        else:
            print(f"âš ï¸ Wiki file not found, skipping: {wiki_path}")

        # --- CHAT KISMI ---
        if chat_exists:
            print(f"ðŸ’¬ Using chat data from: {chat_path}")
            with open(chat_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.rstrip("\n")
                    if not line.strip():
                        # boÅŸ satÄ±rlarÄ± da istersen koruyabilirsin
                        # burada sadece tamamen boÅŸ olanlarÄ± atlÄ±yorum
                        continue
                    for _ in range(chat_repeat):
                        fout.write(line + "\n")
                    chat_lines_used += 1
        else:
            print(f"âš ï¸ Chat file not found, skipping: {chat_path}")

    total_lines = wiki_lines_used * wiki_repeat + chat_lines_used * chat_repeat
    print(
        f"âœ… Mixed corpus written to: {out_path}\n"
        f"   wiki_lines_used={wiki_lines_used} (x{wiki_repeat})\n"
        f"   chat_lines_used={chat_lines_used} (x{chat_repeat})\n"
        f"   total_written_lines={total_lines}"
    )


def train(args):
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    print(f"Using device: {device}")

    os.makedirs("outputs/checkpoints", exist_ok=True)
    os.makedirs("outputs/logs", exist_ok=True)

    # -------------------------------
    # 1) Mixed corpus oluÅŸtur
    # -------------------------------
    print("ðŸ”€ Building mixed corpus (wiki x3 + chat x1)...")
    build_mixed_corpus(
        wiki_path=args.wiki_path,
        chat_path=args.chat_path,
        out_path=args.data_path,
        wiki_repeat=3,
        chat_repeat=1,
        wiki_limit_lines=args.limit_lines,
    )

    # Tokenizer yÃ¼kle
    tokenizer_path = os.path.join("outputs", "tokenizer", "tokenizer.json")
    tokenizer = WordTokenizer.load(tokenizer_path)

    # Dataset
    print(f"Loading dataset from: {args.data_path}")
    dataset = LMTextDataset(
        path=args.data_path,
        tokenizer=tokenizer,
        max_len=args.max_len,
        limit_lines=None,  # limit'i mixed corpus oluÅŸtururken uyguladÄ±k
    )
    print(f"Total samples in dataset: {len(dataset)}")

    # Train / Val split
    val_size = max(1, int(0.05 * len(dataset)))
    train_size = len(dataset) - val_size
    train_ds, val_ds = random_split(dataset, [train_size, val_size])
    print(f"Train samples: {train_size}, Val samples: {val_size}")

    # DataLoader'lar
    train_loader = DataLoader(
        train_ds,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=lambda b: collate_batch(b, pad_id=0),
        num_workers=0,  # Windows iÃ§in gÃ¼venli
        pin_memory=use_cuda,
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=lambda b: collate_batch(b, pad_id=0),
        num_workers=0,
        pin_memory=use_cuda,
    )

    # Model config & model
    config = MiniGPTConfig(
        vocab_size=tokenizer.vocab_size,
        d_model=args.d_model,
        n_heads=args.n_heads,
        n_layers=args.n_layers,
        d_ff=args.d_ff,
        max_len=args.max_len,
        dropout=args.dropout,
    )
    model = MiniGPT(config).to(device)

    optimizer = AdamW(model.parameters(), lr=args.lr)

    # Yeni torch.amp API'si
    if use_cuda:
        scaler = torch.amp.GradScaler("cuda")
        print("Using torch.amp GradScaler (cuda)")
    else:
        scaler = None
        print("CUDA not available, training on CPU without AMP")

    # Checkpoint yollarÄ±
    ckpt_last_path = os.path.join("outputs", "checkpoints", "train_state_last.pt")
    ckpt_best_path = os.path.join("outputs", "checkpoints", "model_best.pt")

    best_val_loss = float("inf")
    start_epoch = 1

    # Log dosyasÄ± (resume ise eski log'u koru)
    log_path = os.path.join("outputs", "logs", "train_log.csv")
    if args.resume and os.path.exists(log_path):
        print(f"ðŸ“„ Existing log found at {log_path}, appending.")
    else:
        with open(log_path, "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["epoch", "train_loss", "val_loss"])

    # EÄŸer resume isteniyorsa ve checkpoint varsa, ordan devam et
    if args.resume and os.path.exists(ckpt_last_path):
        print(f"ðŸ” Resuming training from {ckpt_last_path}")
        ckpt = torch.load(ckpt_last_path, map_location=device)

        # Model aÄŸÄ±rlÄ±klarÄ±
        model.load_state_dict(ckpt["model_state"])

        # Optimizer durumu (varsa)
        if "optimizer_state" in ckpt:
            optimizer.load_state_dict(ckpt["optimizer_state"])

        # Scaler durumu (GPU + kayÄ±tlÄ±ysa)
        if scaler is not None and "scaler_state" in ckpt and ckpt["scaler_state"] is not None:
            try:
                scaler.load_state_dict(ckpt["scaler_state"])
            except Exception as e:
                print(f"Warning: Could not load scaler state: {e}")

        # Epoch ve en iyi val loss
        start_epoch = ckpt.get("epoch", 0) + 1
        best_val_loss = ckpt.get("best_val_loss", float("inf"))

        print(f"âœ… Resumed from epoch {start_epoch - 1}, best_val_loss={best_val_loss:.4f}")
    else:
        if args.resume:
            print("â„¹ï¸ Resume is True but no checkpoint found, starting from scratch.")
        else:
            print("Starting fresh training (no resume).")

    # EÄŸer zaten tÃ¼m epochlar bitmiÅŸse, uyar
    if start_epoch > args.epochs:
        print(
            f"Nothing to do: start_epoch={start_epoch} > total epochs={args.epochs}. "
            f"Increase args.epochs if you want to continue training."
        )
        return

    # Epoch dÃ¶ngÃ¼sÃ¼
    for epoch in range(start_epoch, args.epochs + 1):
        epoch_start_time = time.time()

        model.train()
        total_loss = 0.0
        total_tokens = 0

        print(f"\n===== Epoch {epoch}/{args.epochs} =====")
        num_batches = len(train_loader)

        for step, (x, y) in enumerate(train_loader, start=1):
            x = x.to(device)
            y = y.to(device)

            optimizer.zero_grad()

            if use_cuda:
                # Yeni API: torch.amp.autocast("cuda", ...)
                with torch.amp.autocast("cuda"):
                    _, loss = model(x, y)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                _, loss = model(x, y)
                loss.backward()
                optimizer.step()

            tokens = (y != -100).sum().item()
            total_loss += loss.item() * tokens
            total_tokens += tokens

            # Her N adÄ±mda bir log
            if step % args.log_interval == 0 or step == num_batches:
                avg_loss = total_loss / max(1, total_tokens)
                try:
                    ppl = math.exp(avg_loss)
                except OverflowError:
                    ppl = float("inf")
                print(
                    f"Epoch {epoch} | Step {step}/{num_batches} "
                    f"| batch_loss={loss.item():.4f} | avg_loss={avg_loss:.4f} | ppl={ppl:.2f}"
                )

        train_loss = total_loss / max(1, total_tokens)

        # Validation
        model.eval()
        val_loss_total = 0.0
        val_tokens = 0
        with torch.no_grad():
            for x, y in val_loader:
                x = x.to(device)
                y = y.to(device)
                _, loss = model(x, y)
                tokens = (y != -100).sum().item()
                val_loss_total += loss.item() * tokens
                val_tokens += tokens

        # Epoch sonu validation metriÄŸi
        val_loss = val_loss_total / max(1, val_tokens)
        ppl = math.exp(val_loss)

        # GPU sÄ±caklÄ±ÄŸÄ± ve epoch sÃ¼resi
        gpu_temp = nvmlDeviceGetTemperature(handle, NVML_TEMPERATURE_GPU)
        epoch_duration = time.time() - epoch_start_time

        # Konsola Ã¶zet yaz
        print(
            f"Epoch {epoch} DONE | Train loss: {train_loss:.4f} "
            f"| Val loss: {val_loss:.4f} | Val ppl={ppl:.2f} "
            f"| GPU Temp={gpu_temp}Â°C | Duration={epoch_duration:.1f}s"
        )

        # Log'a yaz
        with open(log_path, "a", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([epoch, train_loss, val_loss])

        # En iyi modeli ayrÄ± kaydet
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(
                {
                    "model_state": model.state_dict(),
                    "config": config.__dict__,
                },
                ckpt_best_path,
            )
            print(f"ðŸ† Saved new best model to {ckpt_best_path}")

        # Her epoch sonunda son durumu (resume iÃ§in) kaydet
        state = {
            "model_state": model.state_dict(),
            "optimizer_state": optimizer.state_dict(),
            "scaler_state": scaler.state_dict() if scaler is not None else None,
            "config": config.__dict__,
            "epoch": epoch,
            "best_val_loss": best_val_loss,
        }
        torch.save(state, ckpt_last_path)
        print(f"ðŸ’¾ Saved last training state to {ckpt_last_path}")


if __name__ == "__main__":
    # ArgÃ¼manlarla uÄŸraÅŸmamak iÃ§in sabit ayarlar
    class Args:
        # Veri ve model ayarlarÄ±
        wiki_path = "data/wiki_clean_processed.txt"
        chat_path = "data/turkish_chat_casual.txt"
        data_path = "data/mixed_wiki_chat.txt"  # build_mixed_corpus Ã§Ä±ktÄ±sÄ±

        limit_lines = 1_500_000  # WIKI iÃ§in satÄ±r limiti
        max_len = 256            # Maksimum token uzunluÄŸu

        # EÄŸitim ayarlarÄ±
        batch_size = 64
        epochs = 10
        lr = 3e-4

        # Model boyutlarÄ±
        d_model = 512
        n_heads = 8
        n_layers = 8
        d_ff = 2048
        dropout = 0.1

        # Log ve resume
        log_interval = 250  # KaÃ§ batch'te bir log basÄ±lsÄ±n
        resume = True      # Checkpoint varsa kaldÄ±ÄŸÄ± yerden devam et

    args = Args()
    train(args)
