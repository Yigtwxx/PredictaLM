import os
import sys
import webbrowser
from contextlib import asynccontextmanager

import torch
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import uvicorn

# =====================================================
#   DB IMPORT (sqlite + sqlalchemy)
#   - db klasÃ¶rÃ¼ PredictaLM kÃ¶kÃ¼nde: PredictaLM/db/...
#   - app.py: PredictaLM/src/app.py
#   KÃ¶k dizini sys.path'e ekleyip db'yi gÃ¶rÃ¼yoruz.
# =====================================================

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))   # .../PredictaLM/src
PROJECT_ROOT = os.path.dirname(CURRENT_DIR)                # .../PredictaLM

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from db.session import SessionLocal, engine
from db.models import GenerationLog, SavedItem

# Tablonun oluÅŸmasÄ±nÄ± saÄŸla (Base import etmeden)
# Tablonun oluÅŸmasÄ±nÄ± saÄŸla (Base import etmeden)
GenerationLog.__table__.create(bind=engine, checkfirst=True)
SavedItem.__table__.create(bind=engine, checkfirst=True)

# =====================================================
#   MODEL / TOKENIZER IMPORT
# =====================================================

from tokenizer import WordTokenizer
from model import MiniGPT, MiniGPTConfig


# =====================================
#   REQUEST / RESPONSE MODELLERÄ°
# =====================================

class CompleteRequest(BaseModel):
    prompt: str
    max_new_tokens: int = 64


class CompleteResponse(BaseModel):
    next_word: str
    full_completion: str


class GenerateResponse(BaseModel):
    output: str


class SaveRequest(BaseModel):
    prompt: str
    completion: str


class SavedItemResponse(BaseModel):
    id: int
    prompt: str
    completion: str
    created_at: str


# =====================================
#   MODEL LOAD (GLOBAL)
# =====================================

_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
_model = None
_tokenizer = None


def _load_model():
    """Model ve tokenizer'Ä± lazy ÅŸekilde yÃ¼kler."""
    global _model, _tokenizer

    if _model is not None:
        return

    print("ğŸ”„ Model yÃ¼kleniyor...")

    ckpt_path = os.path.join("outputs", "checkpoints", "model_best.pt")
    tok_path = os.path.join("outputs", "tokenizer", "tokenizer.json")

    if not os.path.exists(ckpt_path):
        raise RuntimeError(f"âŒ Checkpoint bulunamadÄ±: {ckpt_path}")

    if not os.path.exists(tok_path):
        raise RuntimeError(f"âŒ Tokenizer bulunamadÄ±: {tok_path}")

    # Checkpoint'i yÃ¼kle
    ckpt = torch.load(ckpt_path, map_location="cpu")
    config = MiniGPTConfig(**ckpt["config"])
    model = MiniGPT(config)
    model.load_state_dict(ckpt["model_state"])
    model.to(_device)
    model.eval()

    # Tokenizer
    tokenizer = WordTokenizer.load(tok_path)

    _model = model
    _tokenizer = tokenizer

    print("âœ… Model ve tokenizer yÃ¼klendi!")


def _generate_internal(prompt: str, max_new_tokens: int):
    """
    Hem /complete hem /generate iÃ§in ortak inference fonksiyonu.
    next_word ve full_completion dÃ¶ner.
    """
    _load_model()

    prompt = prompt.strip()
    if not prompt:
        return "", ""

    # Tokenize giriÅŸ
    ids = _tokenizer.encode(prompt)
    if len(ids) == 0:
        return "", ""

    input_ids = torch.tensor([ids], dtype=torch.long, device=_device)

    with torch.no_grad():
        out_ids = _model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_k=50,
        )

    # Ã‡Ä±ktÄ±yÄ± decode et
    out_ids = out_ids[0].tolist()
    full_text = _tokenizer.decode(out_ids)

    # Sadece prompt sonrasÄ± ilk yeni kelimeyi hesapla
    prompt_tokens = _tokenizer.encode(prompt)
    new_ids = out_ids[len(prompt_tokens):]
    next_word = _tokenizer.decode(new_ids[:1]) if new_ids else ""

    print(f"[INFER] prompt='{prompt}' | next_word='{next_word}'")

    return next_word, full_text


# =====================================
#   FASTAPI APP + LIFESPAN
# =====================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Uygulama baÅŸlarken modeli yÃ¼kle
    _load_model()
    print("ğŸš€ API hazÄ±r!")
    yield
    # Shutdown'da ekstra iÅŸ yok


app = FastAPI(title="PredictaLM API", lifespan=lifespan)

# CORS (UI'den istek atmak iÃ§in)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# =====================================
#   API ENDPOINTLERÄ°
# =====================================

@app.post("/complete", response_model=CompleteResponse)
def complete(req: CompleteRequest):
    """
    Metni tamamlayan endpoint (next_word + full_completion).
    UI'de "sonraki kelime" gibi ince bir gÃ¶sterim iÃ§in ideal.
    """
    next_word, full_text = _generate_internal(req.prompt, req.max_new_tokens)
    return CompleteResponse(next_word=next_word, full_completion=full_text)


@app.post("/generate", response_model=GenerateResponse)
async def generate_endpoint(req: CompleteRequest):
    """
    UI'nin kullandÄ±ÄŸÄ± basit endpoint.
    Sadece full_completion dÃ¶ner -> {"output": "..."}
    Ek olarak: SQLite'da generation_logs tablosuna kaydediyoruz.
    """
    _, full_text = _generate_internal(req.prompt, req.max_new_tokens)

    # ==== DB'ye LOG YAZ (SQLite) ====
    db = SessionLocal()
    try:
        log = GenerationLog(
            prompt=req.prompt,
            completion=full_text,
            is_correct=None,  # Ä°leride feedback eklersen gÃ¼ncelleyebilirsin
        )
        db.add(log)
        db.commit()
    finally:
        db.close()
    # ================================

    return GenerateResponse(output=full_text)


@app.post("/save")
def save_item(req: SaveRequest):
    """
    KullanÄ±cÄ±nÄ±n beÄŸendiÄŸi bir Ã§Ä±ktÄ±yÄ± kaydeder.
    """
    db = SessionLocal()
    try:
        item = SavedItem(
            prompt=req.prompt,
            completion=req.completion
        )
        db.add(item)
        db.commit()
        db.refresh(item)
        return {"status": "ok", "id": item.id}
    finally:
        db.close()
@app.get("/saved", response_model=list[SavedItemResponse])
def get_saved_items():
    """
    Kaydedilen tÃ¼m Ã¶ÄŸeleri dÃ¶ner (en yeniden eskiye).
    """
    db = SessionLocal()
    try:
        items = db.query(SavedItem).order_by(SavedItem.created_at.desc()).all()
        # Pydantic modeline uygun formata Ã§evir
        return [
            SavedItemResponse(
                id=item.id,
                prompt=item.prompt,
                completion=item.completion,
                created_at=str(item.created_at)
            )
            for item in items
        ]
    finally:
        db.close()

@app.delete("/saved/{item_id}")
async def delete_saved_item(item_id: int):
    db = SessionLocal()
    try:
        item = db.query(SavedItem).filter(SavedItem.id == item_id).first()
        if not item:
            return {"error": "Item not found"}
        db.delete(item)
        db.commit()
        return {"message": "Deleted"}
    finally:
        db.close()


# =====================================
#   UI SERVE (STATIC FILES)
# =====================================

BASE_DIR = os.path.dirname(__file__)   # .../src
ROOT_DIR = os.path.dirname(BASE_DIR)   # .../PredictaLM

candidate_dirs = [
    os.path.join(BASE_DIR, "ui"),
    os.path.join(ROOT_DIR, "ui"),
]

UI_DIR = None
for d in candidate_dirs:
    if os.path.isdir(d):
        UI_DIR = d
        break

if UI_DIR is None:
    raise RuntimeError(
        "UI klasÃ¶rÃ¼ bulunamadÄ±. Åu konumlara baktÄ±m:\n" +
        "\n".join(candidate_dirs)
    )

print(f"ğŸ–¥ï¸  UI klasÃ¶rÃ¼: {UI_DIR}")

# Statik dosyalarÄ± /ui altÄ±na mount ediyoruz ki
# /generate ve /complete endpointlerini ezmesin.
app.mount("/ui", StaticFiles(directory=UI_DIR, html=True), name="ui")


# =====================================
#   RUN SERVER + AUTO OPEN BROWSER
# =====================================

if __name__ == "__main__":
    PORT = 7860
    URL = f"http://localhost:{PORT}/ui"

    print(f"\nğŸŒ TarayÄ±cÄ± aÃ§Ä±lÄ±yor: {URL}\n")
    webbrowser.open(URL)

    uvicorn.run(
        app,
        host="127.0.0.1",
        port=PORT,
        reload=False,
    )
